<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring-cloud分布式日志采集2-ELK服务的搭建]]></title>
    <url>%2F2019%2F09%2F26%2FSpring-cloud%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%862-ELK%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Elasticsearch是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能；是一套开放REST和JAVA API等结构提供高效搜索功能，可扩展的分布式系统。它构建于Apache Lucene搜索引擎库之上。Logstash是一个用来搜集、分析、过滤日志的工具。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。它可以从许多来源接收日志，这些来源包括 syslog、消息传递（例如 RabbitMQ）和JMX，它能够以多种方式输出数据，包括电子邮件、websockets和Elasticsearch。Kibana是一个基于Web的图形界面，用于搜索、分析和可视化存储在 Elasticsearch指标中的日志数据。它利用Elasticsearch的REST接口来检索数据，不仅允许用户创建他们自己的数据的定制仪表板视图，还允许他们以特殊的方式查询和过滤数据. 下载地址：https://www.elastic.co/cn/downloads/我们需要下载Elasticsearch、Kibana、Logstash，实验阶段我们都只会演示单点部署 Elasticsearch单点安装与部署环境和安装包准备 操作系统：Ubuntu server 16 JDK版本：1.8.0_25，注：jdk版本必须1.8及以上 elasticsearch版本：elasticsearch-6.8.3 安装启动修改elasticsearch.yml1234567891011121314151617#集群名称cluster.name: elastic-search#节点名称node.name: elasticsearch-node-1#data存放的路径path.data: /home/shumei/project/elk/data/es-data#logs日志的路径path.logs: /home/shumei/project/elk/data/es-log#配置内存使用用交换分区,生产环境此项可以设置为true，提高io性能bootstrap.memory_lock: true#监听的网络地址network.host: 0.0.0.0#开启监听的端口http.port: 9200#增加新的参数，这样head插件可以访问es (5.x版本，如果没有可以自己手动加)http.cors.enabled: truehttp.cors.allow-origin: "*" 修改jvm参数12-Xms512m-Xmx512m 启动elasticsearch前台执行: ./elasticsearch后台执行：./elasticsearch -d 刨坑此坑非坑，执行之后发现es启动不了报错信息如下： 123456789101112131415161718[2019-09-27T14:01:55,934][INFO ][o.e.x.s.a.s.FileRolesStore] [elasticsearch-node-1] parsed [0] roles from file [/home/shumei/project/elk/elasticsearch-6.8.3/config/roles.yml][2019-09-27T14:01:56,586][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elasticsearch-node-1] [controller/24070] [Main.cc@109] controller (64 bit): Version 6.8.3 (Build 7ace96ffff9215) Copyright (c) 2019 Elasticsearch BV[2019-09-27T14:01:57,015][DEBUG][o.e.a.ActionModule ] [elasticsearch-node-1] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[2019-09-27T14:01:57,210][INFO ][o.e.d.DiscoveryModule ] [elasticsearch-node-1] using discovery type [zen] and host providers [settings][2019-09-27T14:01:58,134][INFO ][o.e.n.Node ] [elasticsearch-node-1] initialized[2019-09-27T14:01:58,134][INFO ][o.e.n.Node ] [elasticsearch-node-1] starting ...[2019-09-27T14:01:58,288][INFO ][o.e.t.TransportService ] [elasticsearch-node-1] publish_address &#123;10.10.1.7:9300&#125;, bound_addresses &#123;[::]:9300&#125;[2019-09-27T14:01:58,304][INFO ][o.e.b.BootstrapChecks ] [elasticsearch-node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks[2019-09-27T14:01:58,308][ERROR][o.e.b.Bootstrap ] [elasticsearch-node-1] node validation exception[2] bootstrap checks failed[1]: memory locking requested for elasticsearch process but memory is not locked[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144][2019-09-27T14:01:58,314][INFO ][o.e.n.Node ] [elasticsearch-node-1] stopping ...[2019-09-27T14:01:58,352][INFO ][o.e.n.Node ] [elasticsearch-node-1] stopped[2019-09-27T14:01:58,352][INFO ][o.e.n.Node ] [elasticsearch-node-1] closing ...[2019-09-27T14:01:58,362][INFO ][o.e.n.Node ] [elasticsearch-node-1] closed[2019-09-27T14:01:58,363][INFO ][o.e.x.m.p.NativeController] [elasticsearch-node-1] Native controller process has stopped - no new native processes can be started[2019-09-27T14:06:03,655][ERROR][o.e.b.Bootstrap ] [elasticsearch-node-1] Exception 错误1： memory locking requested for elasticsearch process but memory is not locked 原因：6.8.3版本bootstrap.memory_lock此项默认为true，此项配置意义是锁定内存地址，防止es内存被交换出去，也就是避免使用swap交换分区，频繁的交换会导致IOPS变高解决方法1：bootstrap.memory_lock设置为false，此项clear解决方法2：锁定内存地址需要权限和服务期支持，以linux为例，首先切换到root用户，做出以下修改:1、修改/etc/security/limits.conf，文件最后添加以下内容： 123456* soft nofile 65536* hard nofile 65536* soft nproc 32000* hard nproc 32000* hard memlock unlimited* soft memlock unlimited 2、修改/etc/systemd/system.conf，分别修改以下内容： 123DefaultLimitNOFILE=65536DefaultLimitNPROC=32000DefaultLimitMEMLOCK=infinity 3、执行以下操作，立即生效 script1systemctl daemon-reload 错误2：max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 原因：elasticsearch用户拥有的虚拟内存权限太小，至少需要262144解决方法： 123456789101112切换到root用户执行命令：sysctl -w vm.max_map_count=262144查看结果：sysctl -a|grep vm.max_map_count显示：vm.max_map_count = 262144上述方法修改之后，如果重启虚拟机将失效，所以：解决办法：在 /etc/sysctl.conf文件最后添加一行vm.max_map_count=262144即可永久修改 安装elasticsearch-head插件项目地址：https://github.com/mobz/elasticsearch-head/node环境和配置不一一赘述操作安装官方文档来即可 1234安装命令npm install启动命令npm run start elasticsearch-head界面 Kibana安装和启动 直接解压安装包 tar -zxvf kibana-6.8.3-linux-x86_64.tar.gz 修改配置文件kibana-6.8.3-linux-x86_64/config/kibana.yml 12345678#端口server.port: 5601##主机server.host: "10.10.1.7"##es的地址elasticsearch.url: "http://10.10.1.7:9200"##kibana在es中的索引kibana.index: ".kibana" 执行启动命令 1nohup ./kibana &amp;]]></content>
      <categories>
        <category>Spring-cloud分布式日志采集</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>elasticsearch-head</tag>
        <tag>logstash</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-cloud分布式日志采集1-Zipkin服务端和客户端配置]]></title>
    <url>%2F2019%2F09%2F24%2FSpring-cloud%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%861-Zipkin%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于微服务架构中每个服务都可能分散在不通的服务器，因此需要一套分布式的日志解决方案，Spring-cloud提供了一个用来trace服务的组件sleuth，它可以通过日志获得服务的依赖关系。基于sleuth，可以通过现有的日志工具实现分布式日志的采集。我们将要使用的是ELK，也就是elasticsearch、logstash、kibana。首先我们来了解一下zipkin的配置 Zipkin服务端和客户端的配置Zipkin服务端Zipkin是一种分布式跟踪系统。它有助于收集解决微服务架构中延迟问题所需的时序数据。它管理这些数据的收集和查找。Zipkin的设计基于Google Dapper论文。应用程序用于向Zipkin报告时间数据。Zipkin用户界面还提供了一个依赖关系图，显示每个应用程序有多少跟踪请求。如果您正在解决延迟问题或错误问题，则可以根据应用程序，跟踪长度，注释或时间戳过滤或排序所有跟踪。选择跟踪后，您可以看到每个跨度所需的总跟踪时间百分比，从而可以识别问题应用程序。SpringBoot2.0官方已经默认不在支持自建zipkin-server的方式进行服务链路追踪，目前的参考资料和相关建议比较少，经过个人实现，发现zipkin中不少版本的jar包和springboot的版本有冲突出现了不兼容启动不了的情况，但是还是搭建了一个基于springboot启动的zipkin-server,我使用框架版本如下: Springboot 2.1.4.RELEASE SpringCloud Greenwich.SR1 新建common-zipkin服务端配置POM文件，引入版本2.12.3，目前最新版本2.12.9，但是最新版本启动不了 12345678910&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;version&gt;2.12.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;version&gt;2.12.3&lt;/version&gt;&lt;/dependency&gt; application.yml配置 123456789101112131415161718192021server: port: 9797 servlet: context-path: /spring: application: name: common-zipkin main: allow-bean-definition-overriding: trueeureka: instance: prefer-ip-address: true instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; client: service-url: defaultZone: http://10.10.1.7:43274/common-eureka/eureka/management: metrics: web: server: auto-time-requests: false 新建zipkin客户端POM文件引入 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; yml配置 123456spring: sleuth: sampler: rate: 1 zipkin: base-url: http://localhost:9797 Zipkin WEB控制台 zipkin-server控制台首页，查询输入http.path=/account/user/info zipkin单条log详情 思考：1、采样率设置为1，生产环境该如何配置？2、zipkin虽为异步写入，但是默认使用的是http协议写入数据，效率势必会有影响，有哪些方法可以提高效率？3、zipkin-server如何实现高可用，服务重启或者停止未写入的数据如何处理]]></content>
      <categories>
        <category>Spring-cloud分布式日志采集</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>logstash</tag>
        <tag>kibana</tag>
        <tag>zipkin</tag>
        <tag>sleuth</tag>
      </tags>
  </entry>
</search>
