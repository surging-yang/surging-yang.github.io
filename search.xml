<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker学习（2）-Docker镜像]]></title>
    <url>%2F2019%2F10%2F25%2FDocker%E5%AD%A6%E4%B9%A0%EF%BC%882%EF%BC%89-Docker%E9%95%9C%E5%83%8F%2F</url>
    <content type="text"><![CDATA[加速器国内从Dockers Hub拉取镜像有时会遇到困难，此时可以配置镜像加速器。Docker官方和国内很多云服务商都提供了国内加速器服务，例如： docker中国官方镜像 阿里云加速器 Dao Cloud加速器 配置镜像加速器修改docker守护进程/etc/docker/daemon.json123&#123; "registry-mirrors": ["https://registry.docker-cn.com"]&#125; 执行命令 12sudo systemctl daemon-reloadsudo systemctl restart docker.service 下载镜像 12345sudo docker pull tomcatsudo docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat latest 882487b8be1d 6 days ago 507MBhello-world latest fce289e99eb9 9 months ago 1.84kB 启动镜像 1sudo docker run 8080:8080 tomcat]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>镜像加速器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker学习（1）-Debian发行版中安装docker]]></title>
    <url>%2F2019%2F10%2F16%2FDocker%E5%AD%A6%E4%B9%A0%EF%BC%881%EF%BC%89-Debian%E5%8F%91%E8%A1%8C%E7%89%88%E4%B8%AD%E5%AE%89%E8%A3%85docker%2F</url>
    <content type="text"><![CDATA[Docker简介Docker是一个能够把开发的应用程序自动部署到容器的开源引擎。由docker公司开发（www.docker.com,前dotCloud公司，PaaS市场中的老牌提供商）的团队编写，基于Apache 2.0开源协议发行，这里不是科普文，如果是新手无基础想了解更多的docker相关的介绍，推荐《第一本Docker书》作为docker的启蒙书籍，闲话不多说我们来开始dockers安装Debain的发行版有很多，最常见的是Ubuntu，不过我们机器安装的是深度Linux（轻喷），支持国产，人人有责。 安装准备环境检查 1、运行64位CPU构架的计算机（目前只能是x86_64和amd64），请注意，Docker目前不支持32位CPU 2、运行Linux 3.8或更高版本内核。一些老版本的2.6.x或其后的内核也能够运行Docker，但运行结果会有很大的不同。而且，如果需要就老版本内核寻求帮助，通常大家会被建议升级到更高版本的内核 3、内核必须支持一种适合的存储驱动（storage driver），例如： Device Manager[12]； AUFS[13]； vfs[14]； btrfs[15]； ZFS（在Docker 1.7中引入）； 默认存储驱动通常是Device Mapper 4、内核必须支持并开启cgroup[16]和命名空间[17]（namespace）功能 系统内核检查123# 系统内核检查surging@surging-yang:~$ uname -aLinux surging-yang 4.15.0-30deepin-generic #31 SMP Fri Nov 30 04:29:02 UTC 2018 x86_64 GNU/Linux 检查Device Mapper这里将使用Device Mapper作为存储驱动。自2.6.9版本的Linux内核开始已经集成了Device Mapper，并且提供了一个将块设备映射到高级虚拟设备的方法。Device Mapper支持“自动精简配置”（thin-provisioning）的概念，可以在一种文件系统中存储多台虚拟设备（Docker镜像中的层）。因此，用Device Mapper作为Docker的存储驱动是再合适不过了。 1234567# 检查Device Mappersurging@surging-yang:~$ ls -l /sys/class/misc/device-mapperlrwxrwxrwx 1 root root 0 10月 16 2019 /sys/class/misc/device-mapper -&gt; ../../devices/virtual/misc/device-mapper# 检查proc中检查Device Mappersudo grep device-mapper /proc/devices253 device-mapper 安装Docker我们这里比较简单，直接执行apt-get install 命令安装最新版的docker社区版，注：docker-ce为社区版，docker-ee为企业版，我的电脑已经安装了，安装过程直接确认即可，傻瓜式安装 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758surging@surging-yang:~$ sudo apt-get install docker-ce正在读取软件包列表... 完成正在分析软件包的依赖关系树 正在读取状态信息... 完成 docker-ce 已经是最新版 (5:19.03.3~3-0~debian-stretch)。升级了 0 个软件包，新安装了 0 个软件包，要卸载 0 个软件包，有 0 个软件包未被升级。# 检查安装surging@surging-yang:~$ sudo docker infoClient: Debug Mode: falseServer: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 19.03.3 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: b34a5c8af56e510852c35414db4c1f4fa6172339 runc version: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 init version: fec3683 Security Options: apparmor seccomp Profile: default Kernel Version: 4.15.0-30deepin-generic Operating System: Deepin 15 OSType: linux Architecture: x86_64 CPUs: 4 Total Memory: 7.71GiB Name: surging-yang ID: PIX7:QCDI:E7DT:7M6D:XIQB:AXHP:GJRS:4M2R:6AYK:PTX6:3J2Y:X7SP Docker Root Dir: /var/lib/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: falseWARNING: No swap limit support 测试你的安装12345678910111213141516171819202122232425262728293031323334353637# 查看docker版本docker --versionDocker version 19.03.3, build a872fc2f86# 常规操作之hello-worldsudo docker run hello-worldUnable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-world1b930d010525: Pull complete Digest: sha256:c3b4ada4687bbaa170745b3e4dd8ac3f194ca95b2d0518b417fb47e5879d9b5fStatus: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly.To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the "hello-world" image from the Docker Hub. (amd64) 3. The Docker daemon created a new container from that image which runs the executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it to your terminal.To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub.docker.com/For more examples and ideas, visit: https://docs.docker.com/get-started/# 查看本地镜像列表sudo docker image lsREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest fce289e99eb9 9 months ago 1.84kB]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>深度Linux</tag>
        <tag>Debian</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-cloud分布式日志采集2-ELK服务的搭建]]></title>
    <url>%2F2019%2F09%2F26%2FSpring-cloud%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%862-ELK%E6%9C%8D%E5%8A%A1%E7%9A%84%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Elasticsearch是实时全文搜索和分析引擎，提供搜集、分析、存储数据三大功能；是一套开放REST和JAVA API等结构提供高效搜索功能，可扩展的分布式系统。它构建于Apache Lucene搜索引擎库之上。Logstash是一个用来搜集、分析、过滤日志的工具。它支持几乎任何类型的日志，包括系统日志、错误日志和自定义应用程序日志。它可以从许多来源接收日志，这些来源包括 syslog、消息传递（例如 RabbitMQ）和JMX，它能够以多种方式输出数据，包括电子邮件、websockets和Elasticsearch。Kibana是一个基于Web的图形界面，用于搜索、分析和可视化存储在 Elasticsearch指标中的日志数据。它利用Elasticsearch的REST接口来检索数据，不仅允许用户创建他们自己的数据的定制仪表板视图，还允许他们以特殊的方式查询和过滤数据. 下载地址：https://www.elastic.co/cn/downloads/我们需要下载Elasticsearch、Kibana、Logstash，实验阶段只会演示单点部署，后面有时间可以做一个Elasticsearch集群部署文档 Elasticsearch单点安装与部署环境和安装包准备 操作系统：Ubuntu server 16 JDK版本：1.8.0_25，注：jdk版本必须1.8及以上 elasticsearch版本：elasticsearch-6.8.3 安装启动修改elasticsearch.yml1234567891011121314151617#集群名称cluster.name: elastic-search#节点名称node.name: elasticsearch-node-1#data存放的路径path.data: /home/shumei/project/elk/data/es-data#logs日志的路径path.logs: /home/shumei/project/elk/data/es-log#配置内存使用用交换分区,生产环境此项可以设置为true，提高io性能bootstrap.memory_lock: true#监听的网络地址network.host: 0.0.0.0#开启监听的端口http.port: 9200#增加新的参数，这样head插件可以访问es (5.x版本，如果没有可以自己手动加)http.cors.enabled: truehttp.cors.allow-origin: "*" 修改jvm参数12-Xms512m-Xmx512m 启动elasticsearch前台执行: ./elasticsearch后台执行：./elasticsearch -d 刨坑此坑非坑，执行之后发现es启动不了报错信息如下： 123456789101112131415161718[2019-09-27T14:01:55,934][INFO ][o.e.x.s.a.s.FileRolesStore] [elasticsearch-node-1] parsed [0] roles from file [/home/shumei/project/elk/elasticsearch-6.8.3/config/roles.yml][2019-09-27T14:01:56,586][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [elasticsearch-node-1] [controller/24070] [Main.cc@109] controller (64 bit): Version 6.8.3 (Build 7ace96ffff9215) Copyright (c) 2019 Elasticsearch BV[2019-09-27T14:01:57,015][DEBUG][o.e.a.ActionModule ] [elasticsearch-node-1] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security[2019-09-27T14:01:57,210][INFO ][o.e.d.DiscoveryModule ] [elasticsearch-node-1] using discovery type [zen] and host providers [settings][2019-09-27T14:01:58,134][INFO ][o.e.n.Node ] [elasticsearch-node-1] initialized[2019-09-27T14:01:58,134][INFO ][o.e.n.Node ] [elasticsearch-node-1] starting ...[2019-09-27T14:01:58,288][INFO ][o.e.t.TransportService ] [elasticsearch-node-1] publish_address &#123;10.10.1.7:9300&#125;, bound_addresses &#123;[::]:9300&#125;[2019-09-27T14:01:58,304][INFO ][o.e.b.BootstrapChecks ] [elasticsearch-node-1] bound or publishing to a non-loopback address, enforcing bootstrap checks[2019-09-27T14:01:58,308][ERROR][o.e.b.Bootstrap ] [elasticsearch-node-1] node validation exception[2] bootstrap checks failed[1]: memory locking requested for elasticsearch process but memory is not locked[2]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144][2019-09-27T14:01:58,314][INFO ][o.e.n.Node ] [elasticsearch-node-1] stopping ...[2019-09-27T14:01:58,352][INFO ][o.e.n.Node ] [elasticsearch-node-1] stopped[2019-09-27T14:01:58,352][INFO ][o.e.n.Node ] [elasticsearch-node-1] closing ...[2019-09-27T14:01:58,362][INFO ][o.e.n.Node ] [elasticsearch-node-1] closed[2019-09-27T14:01:58,363][INFO ][o.e.x.m.p.NativeController] [elasticsearch-node-1] Native controller process has stopped - no new native processes can be started[2019-09-27T14:06:03,655][ERROR][o.e.b.Bootstrap ] [elasticsearch-node-1] Exception 错误1： memory locking requested for elasticsearch process but memory is not locked 原因：6.8.3版本bootstrap.memory_lock此项默认为true，此项配置意义是锁定内存地址，防止es内存被交换出去，也就是避免使用swap交换分区，频繁的交换会导致IOPS变高解决方法1：bootstrap.memory_lock设置为false，此项clear解决方法2：锁定内存地址需要权限和服务期支持，以linux为例，首先切换到root用户，做出以下修改:1、修改/etc/security/limits.conf，文件最后添加以下内容： 123456* soft nofile 65536* hard nofile 65536* soft nproc 32000* hard nproc 32000* hard memlock unlimited* soft memlock unlimited 2、修改/etc/systemd/system.conf，分别修改以下内容： 123DefaultLimitNOFILE=65536DefaultLimitNPROC=32000DefaultLimitMEMLOCK=infinity 3、执行以下操作，立即生效 1systemctl daemon-reload 错误2：max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 原因：elasticsearch用户拥有的虚拟内存权限太小，至少需要262144解决方法： 123456789101112切换到root用户执行命令：sysctl -w vm.max_map_count=262144查看结果：sysctl -a|grep vm.max_map_count显示：vm.max_map_count = 262144上述方法修改之后，如果重启虚拟机将失效，所以：解决办法：在 /etc/sysctl.conf文件最后添加一行vm.max_map_count=262144即可永久修改 安装elasticsearch-head插件项目地址：https://github.com/mobz/elasticsearch-head/node环境和配置不一一赘述操作安装官方文档来即可 1234安装命令npm install启动命令npm run start elasticsearch-head界面 Kibana安装和启动 直接解压安装包 tar -zxvf kibana-6.8.3-linux-x86_64.tar.gz 修改配置文件kibana-6.8.3-linux-x86_64/config/kibana.yml 12345678910#kibana6.8版本自带国际化，只要修改此项为“zh-CN”，界面即为中文显示i18n.locale: "zh-CN"#端口server.port: 5601##主机server.host: "10.10.1.7"##es的地址elasticsearch.url: "http://10.10.1.7:9200"##kibana在es中的索引kibana.index: ".kibana" 执行启动命令 1nohup ./kibana &amp; logstash安装和启动 直接解压安装包 tar -zxvf logstash-6.8.3.tar.gz 在config新增logstash.conf文件 1234567891011121314151617#输入监听，port为端口号，codec为json格式input &#123; tcp &#123; port =&gt; 4560 codec =&gt; json_lines &#125;&#125;#日志输出配置，这里我们将logstash的日志输出到我们部署的单点elasticsearch中去#host即es的地址，支持集群#index即logstash在es中创建的索引名称output &#123; elasticsearch &#123; hosts =&gt; [&quot;10.10.1.7:9200&quot;] index =&gt; &quot;netpune_log&quot; &#125;&#125; 执行启动命令 script1nohup ./logstash -f ../config/logstash.conf &amp; Logback配置我们采用的日志组件为springboot集成的logback组件，原来的系统日志输出可以不用变更继续输出到原有的日志文件中去即可，需要新增一个appender即可，下面给出一个完成的logback文件配置示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!-- 日志级别从低到高分为TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL，如果设置为WARN，则低于WARN的信息都不会输出 --&gt;&lt;!-- scan:当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true --&gt;&lt;!-- scanPeriod:设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒。当scan为true时，此属性生效。默认的时间间隔为1分钟。 --&gt;&lt;!-- debug:当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。 --&gt;&lt;configuration scan="true" scanPeriod="10 seconds"&gt; &lt;!--&lt;include resource="org/springframework/boot/logging/logback/base.xml" /&gt;--&gt; &lt;contextName&gt;logback&lt;/contextName&gt; &lt;!-- name的值是变量的名称，value的值时变量定义的值。通过定义的值会被插入到logger上下文中。定义变量后，可以使“$&#123;&#125;”来使用变量。 --&gt; &lt;property name="log.path" value="/home/neptune/eagle/logs"/&gt; &lt;springProperty scope="context" name="application.name" source="spring.application.name"/&gt; &lt;!-- 彩色日志 --&gt; &lt;!-- 彩色日志依赖的渲染类 --&gt; &lt;conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter"/&gt; &lt;conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter"/&gt; &lt;conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter"/&gt; &lt;!-- 彩色日志格式 --&gt; &lt;property name="CONSOLE_LOG_PATTERN" value="$&#123;CONSOLE_LOG_PATTERN:-%clr(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;)&#123;faint&#125; %clr($&#123;LOG_LEVEL_PATTERN:-%5p&#125;) %clr($&#123;PID:- &#125;)&#123;magenta&#125; %clr(---)&#123;faint&#125; %clr([%15.15t])&#123;faint&#125; %clr(%-40.40logger&#123;39&#125;)&#123;cyan&#125; %clr(%line:)&#123;faint&#125; %m%n$&#123;LOG_EXCEPTION_CONVERSION_WORD:-%wEx&#125;&#125;"/&gt; &lt;!--输出到控制台--&gt; &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt; &lt;!--此日志appender是为开发使用，只配置最底级别，控制台输出的日志级别是大于或等于此级别的日志信息--&gt; &lt;filter class="ch.qos.logback.classic.filter.ThresholdFilter"&gt; &lt;level&gt;DEBUG&lt;/level&gt; &lt;/filter&gt; &lt;encoder&gt; &lt;Pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/Pattern&gt; &lt;!-- 设置字符集 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--输出到文件--&gt; &lt;!-- 时间滚动输出 level为 DEBUG 日志 --&gt; &lt;appender name="DEBUG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;log.path&#125;/log_debug.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; %line - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!-- 日志归档 --&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/debug/log-debug-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录debug级别的 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;debug&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 INFO 日志 --&gt; &lt;appender name="INFO_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;log.path&#125;/log_info.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;!-- 每天日志归档路径以及格式 --&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/info/log-info-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录info级别的 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;info&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 WARN 日志 --&gt; &lt;appender name="WARN_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;log.path&#125;/log_warn.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/warn/log-warn-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录warn级别的 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;warn&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- 时间滚动输出 level为 ERROR 日志 --&gt; &lt;appender name="ERROR_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt; &lt;!-- 正在记录的日志文件的路径及文件名 --&gt; &lt;file&gt;$&#123;log.path&#125;/log_error.log&lt;/file&gt; &lt;!--日志文件输出格式--&gt; &lt;encoder&gt; &lt;pattern&gt;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%thread] %-5level %logger&#123;50&#125; - %msg%n&lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;!-- 此处设置字符集 --&gt; &lt;/encoder&gt; &lt;!-- 日志记录器的滚动策略，按日期，按大小记录 --&gt; &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt; &lt;fileNamePattern&gt;$&#123;log.path&#125;/error/log-error-%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxFileSize&gt;30MB&lt;/maxFileSize&gt; &lt;!--日志文件保留天数--&gt; &lt;maxHistory&gt;15&lt;/maxHistory&gt; &lt;/rollingPolicy&gt; &lt;!-- 此日志文件只记录ERROR级别的 --&gt; &lt;filter class="ch.qos.logback.classic.filter.LevelFilter"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;onMatch&gt;ACCEPT&lt;/onMatch&gt; &lt;onMismatch&gt;DENY&lt;/onMismatch&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender"&gt; &lt;destination&gt;10.10.1.7:4560&lt;/destination&gt; &lt;includeCallerData&gt;true&lt;/includeCallerData&gt; &lt;encoder class="net.logstash.logback.encoder.LogstashEncoder"&gt; &lt;providers&gt; &lt;timestamp&gt; &lt;timeZone&gt;UTC&lt;/timeZone&gt; &lt;/timestamp&gt; &lt;pattern&gt; &lt;pattern&gt; &#123; "severity":"%level", "application": "$&#123;application.name:-&#125;", "trace": "%X&#123;X-B3-TraceId:-&#125;", "span": "%X&#123;X-B3-SpanId:-&#125;", "exportable": "%X&#123;X-Span-Export:-&#125;", "pid": "$&#123;PID:-&#125;", "thread": "%thread", "class": "%logger&#123;40&#125;", "rest": "%message" &#125; &lt;/pattern&gt; &lt;/pattern&gt; &lt;/providers&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!--开发环境:打印控制台--&gt; &lt;springProfile name="info"&gt; &lt;logger name="com.eagle.view" level="info"/&gt; &lt;/springProfile&gt; &lt;root level="info"&gt; &lt;appender-ref ref="logstash"/&gt; &lt;appender-ref ref="CONSOLE"/&gt; &lt;appender-ref ref="DEBUG_FILE"/&gt; &lt;appender-ref ref="INFO_FILE"/&gt; &lt;appender-ref ref="WARN_FILE"/&gt; &lt;appender-ref ref="ERROR_FILE"/&gt; &lt;/root&gt;&lt;/configuration&gt; 环境及访问地址总结zipkin访问地址：http://10.10.1.7:9797/zipkin/elasticsearch地址：http://10.10.1.7:9200elasticsearch-header插件访问地址:http://10.10.1.7:9100kibana访问地址：http://10.10.1.7:5601/app/kibana#/home?_g=()logstash输入监听地址：10.10.1.7 4560 Kibana配置图解 依次点击“管理”-&gt;索引模式-&gt;创建索引模式 点击下一步，选择@timestamp，点击创建索引模式按钮 点击Discover查看索引可以根据zipkin中的traceId获取单次请求的所有日志信息 其他：可以在kibana上面管理es日志索引的清理时间，这里我们配置的是15天有效期]]></content>
      <categories>
        <category>Spring-cloud分布式日志采集</category>
      </categories>
      <tags>
        <tag>elasticsearch</tag>
        <tag>logstash</tag>
        <tag>kibana</tag>
        <tag>elasticsearch-head</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-cloud分布式日志采集1-Zipkin服务端和客户端配置]]></title>
    <url>%2F2019%2F09%2F24%2FSpring-cloud%E5%88%86%E5%B8%83%E5%BC%8F%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%861-Zipkin%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%92%8C%E5%AE%A2%E6%88%B7%E7%AB%AF%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于微服务架构中每个服务都可能分散在不通的服务器，因此需要一套分布式的日志解决方案，Spring-cloud提供了一个用来trace服务的组件sleuth，它可以通过日志获得服务的依赖关系。基于sleuth，可以通过现有的日志工具实现分布式日志的采集。我们将要使用的是ELK，也就是elasticsearch、logstash、kibana。首先我们来了解一下zipkin的配置 Zipkin服务端和客户端的配置Zipkin服务端Zipkin是一种分布式跟踪系统。它有助于收集解决微服务架构中延迟问题所需的时序数据。它管理这些数据的收集和查找。Zipkin的设计基于Google Dapper论文。应用程序用于向Zipkin报告时间数据。Zipkin用户界面还提供了一个依赖关系图，显示每个应用程序有多少跟踪请求。如果您正在解决延迟问题或错误问题，则可以根据应用程序，跟踪长度，注释或时间戳过滤或排序所有跟踪。选择跟踪后，您可以看到每个跨度所需的总跟踪时间百分比，从而可以识别问题应用程序。SpringBoot2.0官方已经默认不在支持自建zipkin-server的方式进行服务链路追踪，目前的参考资料和相关建议比较少，经过个人实现，发现zipkin中不少版本的jar包和springboot的版本有冲突出现了不兼容启动不了的情况，但是还是搭建了一个基于springboot启动的zipkin-server,我使用框架版本如下: Springboot 2.1.4.RELEASE SpringCloud Greenwich.SR1 新建common-zipkin服务端配置POM文件，引入版本2.12.3，目前最新版本2.12.9，但是最新版本启动不了 12345678910&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-server&lt;/artifactId&gt; &lt;version&gt;2.12.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.zipkin.java&lt;/groupId&gt; &lt;artifactId&gt;zipkin-autoconfigure-ui&lt;/artifactId&gt; &lt;version&gt;2.12.3&lt;/version&gt;&lt;/dependency&gt; application.yml配置 123456789101112131415161718192021server: port: 9797 servlet: context-path: /spring: application: name: common-zipkin main: allow-bean-definition-overriding: trueeureka: instance: prefer-ip-address: true instance-id: $&#123;spring.cloud.client.ip-address&#125;:$&#123;server.port&#125; client: service-url: defaultZone: http://10.10.1.7:43274/common-eureka/eureka/management: metrics: web: server: auto-time-requests: false 新建zipkin客户端POM文件引入 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-sleuth&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-sleuth-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; yml配置 123456spring: sleuth: sampler: rate: 1 zipkin: base-url: http://localhost:9797 Zipkin WEB控制台 zipkin-server控制台首页，查询输入http.path=/account/user/info zipkin单条log详情 思考：1、采样率设置为1，生产环境该如何配置？2、zipkin虽为异步写入，但是默认使用的是http协议写入数据，效率势必会有影响，有哪些方法可以提高效率？3、zipkin-server如何实现高可用，服务重启或者停止未写入的数据如何处理]]></content>
      <categories>
        <category>Spring-cloud分布式日志采集</category>
      </categories>
      <tags>
        <tag>zipkin</tag>
        <tag>sleuth</tag>
        <tag>elasticsearch</tag>
        <tag>logstash</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
</search>
